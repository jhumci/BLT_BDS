{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width: 100%\">\n",
    "    <tr style=\"background: #ffffff\">\n",
    "        <td style=\"padding-top:25px;width: 180px\"><img src=\"https://mci.edu/templates/mci/images/logo.svg\" alt=\"Logo\"></td>\n",
    "        <td style=\"width: 100%\">\n",
    "            <div style=\"text-align:right; width: 100%; text-align:right\"><font style=\"font-size:38px\"><b>Data Science</b></font></div>\n",
    "            <div style=\"padding-top:0px; width: 100%; text-align:right\"><font size=\"4\"><b></b></font></div>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "# 9 Image Classification with Deep Learning\n",
    "\n",
    "This notebook is a very short introduction to image classification using pre-trained neural networks and `fastai`. The notebook builds on the [tutorial from Jeremy Howard](https://www.kaggle.com/code/jhoward/is-it-a-bird-creating-a-model-from-your-own-data/notebook). You can read the [post by \n",
    "Hafiz Ahmad Hassan](https://medium.com/@l154359/deep-learning-module-ii-fast-ai-series-image-classification-1-d21be0198aa7) if You are interested in more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used Packages\n",
    "\n",
    "We will use fast.ai. They have the motto of *\"Making neural nets uncool again\"*. To-do so they developed a framework, which makes it very easy to use state of the art deep-learning models. The concept is similar to `seaborn` which abstracts from `matplotlib` to make nicer looking plots more conveniently.\n",
    "\n",
    "We start by installing the following Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: duckduckgo_search in c:\\python312\\lib\\site-packages (3.9.11)\n",
      "Requirement already satisfied: aiofiles>=23.2.1 in c:\\python312\\lib\\site-packages (from duckduckgo_search) (23.2.1)\n",
      "Requirement already satisfied: click>=8.1.7 in c:\\python312\\lib\\site-packages (from duckduckgo_search) (8.1.7)\n",
      "Requirement already satisfied: lxml>=4.9.3 in c:\\python312\\lib\\site-packages (from duckduckgo_search) (4.9.3)\n",
      "Requirement already satisfied: httpx>=0.25.1 in c:\\python312\\lib\\site-packages (from httpx[brotli,http2,socks]>=0.25.1->duckduckgo_search) (0.25.2)\n",
      "Requirement already satisfied: colorama in c:\\python312\\lib\\site-packages (from click>=8.1.7->duckduckgo_search) (0.4.6)\n",
      "Requirement already satisfied: anyio in c:\\python312\\lib\\site-packages (from httpx>=0.25.1->httpx[brotli,http2,socks]>=0.25.1->duckduckgo_search) (4.1.0)\n",
      "Requirement already satisfied: certifi in c:\\python312\\lib\\site-packages (from httpx>=0.25.1->httpx[brotli,http2,socks]>=0.25.1->duckduckgo_search) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\python312\\lib\\site-packages (from httpx>=0.25.1->httpx[brotli,http2,socks]>=0.25.1->duckduckgo_search) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\python312\\lib\\site-packages (from httpx>=0.25.1->httpx[brotli,http2,socks]>=0.25.1->duckduckgo_search) (3.6)\n",
      "Requirement already satisfied: sniffio in c:\\python312\\lib\\site-packages (from httpx>=0.25.1->httpx[brotli,http2,socks]>=0.25.1->duckduckgo_search) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\python312\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.1->httpx[brotli,http2,socks]>=0.25.1->duckduckgo_search) (0.14.0)\n",
      "Requirement already satisfied: brotli in c:\\python312\\lib\\site-packages (from httpx[brotli,http2,socks]>=0.25.1->duckduckgo_search) (1.1.0)\n",
      "Requirement already satisfied: h2<5,>=3 in c:\\python312\\lib\\site-packages (from httpx[brotli,http2,socks]>=0.25.1->duckduckgo_search) (4.1.0)\n",
      "Requirement already satisfied: socksio==1.* in c:\\python312\\lib\\site-packages (from httpx[brotli,http2,socks]>=0.25.1->duckduckgo_search) (1.0.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in c:\\python312\\lib\\site-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.25.1->duckduckgo_search) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in c:\\python312\\lib\\site-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.25.1->duckduckgo_search) (4.0.0)\n",
      "Collecting fastai\n",
      "  Using cached fastai-2.7.13-py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: pip in c:\\python312\\lib\\site-packages (from fastai) (23.3.1)\n",
      "Requirement already satisfied: packaging in c:\\python312\\lib\\site-packages (from fastai) (23.2)\n",
      "Requirement already satisfied: fastdownload<2,>=0.0.5 in c:\\python312\\lib\\site-packages (from fastai) (0.0.7)\n",
      "Requirement already satisfied: fastcore<1.6,>=1.5.29 in c:\\python312\\lib\\site-packages (from fastai) (1.5.29)\n",
      "INFO: pip is looking at multiple versions of fastai to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached fastai-2.7.12-py3-none-any.whl (233 kB)\n",
      "  Using cached fastai-2.7.11-py3-none-any.whl (232 kB)\n",
      "  Using cached fastai-2.7.10-py3-none-any.whl (240 kB)\n",
      "  Using cached fastai-2.7.9-py3-none-any.whl (225 kB)\n",
      "  Using cached fastai-2.7.8-py3-none-any.whl (235 kB)\n",
      "  Using cached fastai-2.7.7-py3-none-any.whl (204 kB)\n",
      "  Using cached fastai-2.7.6-py3-none-any.whl (204 kB)\n",
      "Collecting fastcore<1.5,>=1.4.5 (from fastai)\n",
      "  Using cached fastcore-1.4.5-py3-none-any.whl (61 kB)\n",
      "INFO: pip is still looking at multiple versions of fastai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting fastai\n",
      "  Using cached fastai-2.7.5-py3-none-any.whl (214 kB)\n",
      "  Using cached fastai-2.7.4-py3-none-any.whl (214 kB)\n",
      "  Using cached fastai-2.7.3-py3-none-any.whl (213 kB)\n",
      "  Using cached fastai-2.7.2-py3-none-any.whl (213 kB)\n",
      "  Using cached fastai-2.7.1-py3-none-any.whl (213 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached fastai-2.7.0-py3-none-any.whl (213 kB)\n",
      "  Using cached fastai-2.6.3-py3-none-any.whl (197 kB)\n",
      "  Using cached fastai-2.6.2-py3-none-any.whl (197 kB)\n",
      "  Using cached fastai-2.6.1-py3-none-any.whl (197 kB)\n",
      "  Using cached fastai-2.6.0-py3-none-any.whl (188 kB)\n",
      "  Using cached fastai-2.5.6-py3-none-any.whl (188 kB)\n",
      "  Using cached fastai-2.5.5-py3-none-any.whl (187 kB)\n",
      "  Using cached fastai-2.5.4-py3-none-any.whl (187 kB)\n",
      "Collecting fastcore<1.4,>=1.3.27 (from fastai)\n",
      "  Using cached fastcore-1.3.29-py3-none-any.whl (55 kB)\n",
      "Collecting fastai\n",
      "  Using cached fastai-2.5.3-py3-none-any.whl (189 kB)\n",
      "  Using cached fastai-2.5.2-py3-none-any.whl (186 kB)\n",
      "  Using cached fastai-2.5.1-py3-none-any.whl (188 kB)\n",
      "  Using cached fastai-2.5.0-py3-none-any.whl (188 kB)\n",
      "  Using cached fastai-2.4.1-py3-none-any.whl (188 kB)\n",
      "  Using cached fastai-2.4-py3-none-any.whl (187 kB)\n",
      "  Using cached fastai-2.3.1-py3-none-any.whl (194 kB)\n",
      "  Using cached fastai-2.3.0-py3-none-any.whl (193 kB)\n",
      "  Using cached fastai-2.2.7-py3-none-any.whl (193 kB)\n",
      "  Using cached fastai-2.2.6-py3-none-any.whl (193 kB)\n",
      "  Using cached fastai-2.2.5-py3-none-any.whl (191 kB)\n",
      "  Using cached fastai-2.2.4-py3-none-any.whl (191 kB)\n",
      "  Using cached fastai-2.2.3-py3-none-any.whl (191 kB)\n",
      "  Using cached fastai-2.2.2-py3-none-any.whl (191 kB)\n",
      "  Using cached fastai-2.2.1-py3-none-any.whl (191 kB)\n",
      "  Using cached fastai-2.2.0-py3-none-any.whl (191 kB)\n",
      "  Using cached fastai-2.1.10-py3-none-any.whl (190 kB)\n",
      "  Using cached fastai-2.1.9-py3-none-any.whl (190 kB)\n",
      "  Using cached fastai-2.1.8-py3-none-any.whl (189 kB)\n",
      "  Using cached fastai-2.1.7-py3-none-any.whl (189 kB)\n",
      "  Using cached fastai-2.1.6-py3-none-any.whl (189 kB)\n",
      "  Using cached fastai-2.1.5-py3-none-any.whl (188 kB)\n",
      "  Using cached fastai-2.1.4-py3-none-any.whl (188 kB)\n",
      "  Using cached fastai-2.1.3-py3-none-any.whl (188 kB)\n",
      "  Using cached fastai-2.1.2-py3-none-any.whl (188 kB)\n",
      "  Using cached fastai-2.1.1-py3-none-any.whl (188 kB)\n",
      "  Using cached fastai-2.1.0-py3-none-any.whl (188 kB)\n",
      "  Using cached fastai-2.0.19-py3-none-any.whl (188 kB)\n",
      "  Using cached fastai-2.0.18-py3-none-any.whl (188 kB)\n",
      "Collecting fastcore<1.3,>=1.1 (from fastai)\n",
      "  Using cached fastcore-1.2.5-py3-none-any.whl (45 kB)\n",
      "Collecting fastai\n",
      "  Using cached fastai-2.0.17-py3-none-any.whl (188 kB)\n",
      "  Using cached fastai-2.0.16-py3-none-any.whl (187 kB)\n",
      "  Using cached fastai-2.0.15-py3-none-any.whl (185 kB)\n",
      "  Using cached fastai-2.0.14-py3-none-any.whl (356 kB)\n",
      "  Using cached fastai-2.0.13-py3-none-any.whl (355 kB)\n",
      "  Using cached fastai-2.0.12-py3-none-any.whl (355 kB)\n",
      "  Using cached fastai-2.0.11-py3-none-any.whl (354 kB)\n",
      "  Using cached fastai-2.0.10-py3-none-any.whl (354 kB)\n",
      "  Using cached fastai-2.0.9-py3-none-any.whl (354 kB)\n",
      "  Using cached fastai-2.0.8-py3-none-any.whl (353 kB)\n",
      "  Using cached fastai-2.0.7-py3-none-any.whl (353 kB)\n",
      "  Using cached fastai-2.0.6-py3-none-any.whl (353 kB)\n",
      "  Using cached fastai-2.0.5-py3-none-any.whl (353 kB)\n",
      "  Using cached fastai-2.0.4-py3-none-any.whl (353 kB)\n",
      "  Using cached fastai-2.0.3-py3-none-any.whl (353 kB)\n",
      "  Using cached fastai-2.0.2-py3-none-any.whl (353 kB)\n",
      "  Using cached fastai-2.0.0-py3-none-any.whl (350 kB)\n",
      "  Using cached fastai-1.0.61-py3-none-any.whl (239 kB)\n",
      "Collecting bottleneck (from fastai)\n",
      "  Using cached Bottleneck-1.3.7.tar.gz (103 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: fastprogress>=0.2.1 in c:\\python312\\lib\\site-packages (from fastai) (1.0.3)\n",
      "Collecting beautifulsoup4 (from fastai)\n",
      "  Using cached beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\python312\\lib\\site-packages (from fastai) (3.8.2)\n",
      "Collecting numexpr (from fastai)\n",
      "  Using cached numexpr-2.8.8-cp312-cp312-win_amd64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\python312\\lib\\site-packages (from fastai) (1.26.1)\n",
      "Collecting nvidia-ml-py3 (from fastai)\n",
      "  Using cached nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: pandas in c:\\python312\\lib\\site-packages (from fastai) (2.1.3)\n",
      "Requirement already satisfied: Pillow in c:\\python312\\lib\\site-packages (from fastai) (10.1.0)\n",
      "Requirement already satisfied: pyyaml in c:\\python312\\lib\\site-packages (from fastai) (6.0.1)\n",
      "Collecting requests (from fastai)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: scipy in c:\\python312\\lib\\site-packages (from fastai) (1.11.4)\n",
      "Collecting fastai\n",
      "  Using cached fastai-1.0.60-py3-none-any.whl (237 kB)\n",
      "  Using cached fastai-1.0.59.tar.gz (3.5 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "  Using cached fastai-1.0.58-py3-none-any.whl (236 kB)\n",
      "  Using cached fastai-1.0.57-py3-none-any.whl (233 kB)\n",
      "  Using cached fastai-1.0.55-py3-none-any.whl (230 kB)\n",
      "  Using cached fastai-1.0.54-py3-none-any.whl (229 kB)\n",
      "  Using cached fastai-1.0.53.post3-py3-none-any.whl (229 kB)\n",
      "  Using cached fastai-1.0.53.post2-py3-none-any.whl (229 kB)\n",
      "  Using cached fastai-1.0.53.post1-py3-none-any.whl (229 kB)\n",
      "  Using cached fastai-1.0.53-py3-none-any.whl (228 kB)\n",
      "  Using cached fastai-1.0.52-py3-none-any.whl (219 kB)\n",
      "  Using cached fastai-1.0.51-py3-none-any.whl (214 kB)\n",
      "  Using cached fastai-1.0.50.post1-py3-none-any.whl (212 kB)\n",
      "  Using cached fastai-1.0.50-py3-none-any.whl (210 kB)\n",
      "  Using cached fastai-1.0.49-py3-none-any.whl (209 kB)\n",
      "  Using cached fastai-1.0.48-py3-none-any.whl (208 kB)\n",
      "  Using cached fastai-1.0.47.post1-py3-none-any.whl (205 kB)\n",
      "  Using cached fastai-1.0.47-py3-none-any.whl (205 kB)\n",
      "  Using cached fastai-1.0.46-py3-none-any.whl (193 kB)\n",
      "  Using cached fastai-1.0.44-py3-none-any.whl (175 kB)\n",
      "  Using cached fastai-1.0.43.post1-py3-none-any.whl (175 kB)\n",
      "  Using cached fastai-1.0.42-py3-none-any.whl (162 kB)\n",
      "Collecting spacy>=2.0.18 (from fastai)\n",
      "  Using cached spacy-3.7.2-cp312-cp312-win_amd64.whl.metadata (26 kB)\n",
      "Collecting fastai\n",
      "  Using cached fastai-1.0.41-py3-none-any.whl (164 kB)\n",
      "  Using cached fastai-1.0.40-py3-none-any.whl (163 kB)\n",
      "  Using cached fastai-1.0.39-py3-none-any.whl (150 kB)\n",
      "  Using cached fastai-1.0.38-py3-none-any.whl (148 kB)\n",
      "Collecting cymem==2.0.2 (from fastai)\n",
      "  Using cached cymem-2.0.2.tar.gz (47 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Some build dependencies for cymem==2.0.2 from https://files.pythonhosted.org/packages/8b/dc/0976e04cc46f86e0dd3ee3797ec68057eaafebf31daca9a076dc138b9920/cymem-2.0.2.tar.gz (from fastai) conflict with the backend dependencies: wheel==0.42.0 is incompatible with wheel>=0.32.0,<0.33.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastcore in c:\\python312\\lib\\site-packages (1.5.29)\n",
      "Requirement already satisfied: pip in c:\\python312\\lib\\site-packages (from fastcore) (23.3.1)\n",
      "Requirement already satisfied: packaging in c:\\python312\\lib\\site-packages (from fastcore) (23.2)\n",
      "Requirement already satisfied: fastdownload in c:\\python312\\lib\\site-packages (0.0.7)\n",
      "Requirement already satisfied: fastprogress in c:\\python312\\lib\\site-packages (from fastdownload) (1.0.3)\n",
      "Requirement already satisfied: fastcore>=1.3.26 in c:\\python312\\lib\\site-packages (from fastdownload) (1.5.29)\n",
      "Requirement already satisfied: pip in c:\\python312\\lib\\site-packages (from fastcore>=1.3.26->fastdownload) (23.3.1)\n",
      "Requirement already satisfied: packaging in c:\\python312\\lib\\site-packages (from fastcore>=1.3.26->fastdownload) (23.2)\n"
     ]
    }
   ],
   "source": [
    "# A package to use the duck duck go search engine from within python\n",
    "!pip install duckduckgo_search\n",
    "\n",
    "# The fast.ai library\n",
    "!pip install fastai\n",
    "!pip install fastcore\n",
    "\n",
    "# A library to download pictures easily\n",
    "!pip install fastdownload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Data-Set\n",
    "\n",
    "First we need some images to fine-tune our model. Note that we will use a pre-trained model as the training ot the thousands of weights (parameters) would take to much data an time before we would get good results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Data\n",
    "\n",
    "We will automatically download the pictures matching a search term from the duckduckgo-search engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckduckgo_search import ddg_images\n",
    "from fastcore.all import *\n",
    "\n",
    "def search_images(term, max_images=30):\n",
    "    \"\"\"A function that download the first n pictures of a search term and returns a list of the picture urls\"\"\"\n",
    "    print(f\"Searching for '{term}'\")\n",
    "\n",
    "    return L(ddg_images(term, max_results=max_images)).itemgot('image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for 'bird photos'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\duckduckgo_search\\compat.py:40: UserWarning: ddg_images is deprecated. Use DDGS().images() generator\n",
      "  warnings.warn(\"ddg_images is deprecated. Use DDGS().images() generator\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://images.pexels.com/photos/1661179/pexels-photo-1661179.jpeg?cs=srgb&dl=green-bird-1661179.jpg&fm=jpg'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the function\n",
    "#NB: `search_images` depends on duckduckgo.com, which doesn't always return correct responses.\n",
    "#    If you' get a JSON error, just try running it again (it may take a couple of tries).\n",
    "urls = search_images('bird photos', max_images=1)\n",
    "urls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download the image in the `images`-subfolder auf our directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('data/images/bird.jpg')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastdownload import download_url\n",
    "dest = 'data/images/bird.jpg'\n",
    "download_url(urls[0], dest, show_progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to check the image within the notebook, we can print a thumbnail:\n",
    "\n",
    "You can imagine that the predicted variable $y$ is the label of the image. The label is the name of the folder, where the image is stored. In particular, the label must be dummy encoded for the model to work.\n",
    "\n",
    "The predictor $X$ is the image itself, which the color values of each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mall\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fastai'"
     ]
    }
   ],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18996\\2564634243.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/images/bird.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_thumb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "im = Image.open('data/images/bird.jpg')\n",
    "im.to_thumb(256,256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same works for other search terms. In this example, we want to build a classifier that can spot birds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url(search_images('forest photos', max_images=1)[0], 'data/images/forest.jpg', show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18996\\4197025591.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/images/forest.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_thumb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "Image.open('data/images/forest.jpg').to_thumb(256,256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Data with different Characteristics and Labels\n",
    "\n",
    "We want to make sure, that the training data is divers and we do not use the same type of bird over and over again. For this reason, we do not only download the first pictures of birds which could be comics of angry birds instead of ral photos (which are probably bright with sky in the background) and forests (probably dark), but also include different lightning conditions in the search term.\n",
    "\n",
    "Hence, we adapt our search term `'<bird/forest> photo'` and download multiple pictures with different search terms.\n",
    "\n",
    "In this case, we have a very easy approach to label the data. We just label everything that has `bird` in the search term as a bird and everything with `forest` as a forest. We also do not need to have a list of the labels, as, we just store all birds in the `bird_or_not/bird` subfolder and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classes\n",
    "searches = 'forest','bird'\n",
    "\n",
    "# Define where to store the data\n",
    "path = Path('data/images/bird_or_not')\n",
    "\n",
    "# Sleep time so that the search engine does not block us, because of too many requests\n",
    "from time import sleep\n",
    "\n",
    "# For all class labels\n",
    "for class_label in searches:\n",
    "    # select the folder to store them\n",
    "    dest = (path/class_label)\n",
    "    # create the folder if it does not exists\n",
    "    dest.mkdir(exist_ok=True, parents=True)\n",
    "    # Use the download function to store the picture\n",
    "    download_images(dest, urls=search_images(f'{class_label} photo'))\n",
    "    sleep(10)  # Pause between searches to avoid over-loading server\n",
    "    download_images(dest, urls=search_images(f'{class_label} sun photo'))\n",
    "    sleep(10)\n",
    "    download_images(dest, urls=search_images(f'{class_label} shade photo'))\n",
    "    sleep(10)\n",
    "    resize_images(path/class_label, max_size=400, dest=path/class_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do a quick check to remove corrupted files and remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = verify_images(get_image_files(path))\n",
    "failed.map(Path.unlink)\n",
    "len(failed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Data Block for the Model to train with\n",
    "\n",
    "Instead of two Dataframes (`X` for the features and `y` for the predicted variable) fast.ai uses a [DataBlock-Objekt](https://docs.fast.ai/data.block.html#datablock), to describe all the data to be loaded for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock),         # Predictor (items) are images, predicted variable are categories\n",
    "    get_items=get_image_files,                  # The images must be loaded as image\n",
    "    splitter=RandomSplitter(valid_pct=0.2, seed=42),    # Use 20% of randomly selected pictures for validation\n",
    "    get_y=parent_label,                         # To get the y-labels use the name of the parent directory\n",
    "    item_tfms=[Resize(192, method='squish')]    # Transform the data by resizing them to 192x192 pixels and by squishing them\n",
    ").dataloaders(path, bs=32)                      # Load the date from the path = 'bird_or_not' in a batch size of 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show the data in a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch(max_n=6)                         # Show the first 6 items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending in when You downloaded Your pictures You can see, what training data we created. From the squishing some bird might be a litte fat or too lean. However, the ANN will still be able to work with that. We will also have some faulty pictures like pictures of lamps with birds in this example. Hence, in some cases manual sorting might improve the results. \n",
    "\n",
    "Also note, that we resize the image a lot. This will work fine in this use case, but not in use cases with more focus an detail.\n",
    "\n",
    "![](https://i.imgur.com/E6ZuClP.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "For training we can select a pre-configured [vision learner](https://docs.fast.ai/vision.learner.html#vision_learner) to which we pass our training-and-validation data `dls`. These models have been trained on thousands of images and are able to classify images with a high accuracy. However, we need to fine-tune the model to our specific use case.\n",
    "\n",
    "We also select an error metric like the accuracy and a pre-trained network. [ResNet18](https://arxiv.org/pdf/1512.03385.pdf) is a commonly used model that was pre-trained on thousands of pictures and is rather small compared to other models. Hence, it is a good starting point for our use case.\n",
    "\n",
    "![](https://www.researchgate.net/profile/Muhammed-Enes-Atik-2/publication/349241995/figure/fig2/AS:991139192643586@1613317406497/ResNet-18-architecture-20-The-numbers-added-to-the-end-of-ResNet-represent-the_W640.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select pre-trained model and download it\n",
    "learn = vision_learner(dls, resnet18, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we train the model over a given number of runs through the entire training data set (epochs) until a stopping criteria is reached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "learn.fine_tune(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model\n",
    "\n",
    "Finally we can test the model with the images we downloaded first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction\n",
    "Image.open('data/images/bird.jpg').to_thumb(256,256)\n",
    "predicted_class,_,probs = learn.predict(PILImage.create('data/images/bird.jpg'))\n",
    "print(f\"This is a: {predicted_class}.\")\n",
    "print(f\"Probability it's a bird: {probs[0]:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction\n",
    "Image.open('data/images/forest.jpg').to_thumb(256,256)\n",
    "predicted_class,_,probs = learn.predict(PILImage.create('data/images/forest.jpg'))\n",
    "print(f\"This is a: {predicted_class}.\")\n",
    "print(f\"Probability it's a bird: {probs[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url(search_images('Donald Trump', max_images=1)[0], 'data/images/trump.jpg', show_progress=False)\n",
    "Image.open('data/images/trump.jpg').to_thumb(256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction\n",
    "predicted_class,_,probs = learn.predict(PILImage.create('data/images/trump.jpg'))\n",
    "print(f\"This is a: {predicted_class}.\")\n",
    "print(f\"Probability it's a bird: {probs[0]:.4f}\")\n",
    "print(f\"Probability it's a forest: {probs[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍️ **Task**\n",
    "\n",
    "- think of another use case for image classification\n",
    "- copy the notebook and adapt it to Your use case\n",
    "- e.g., classify images of different pengiun species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fcbd7240ee8f908d933dc7f71e8c42a1a91163b70ede8dcff5146d4087436c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
